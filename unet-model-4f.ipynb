{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment Variables and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, Sequential, InstanceNorm2d, ReLU, Dropout2d, Module, ModuleList, functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import compare_ssim \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs\n",
    "data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_test = '/data/local/NC2019MRI/test'\n",
    "\n",
    "# CHANGE OUTPUT DIRECTORY - output directory for test images\n",
    "out_dir = \"/tmp/bhm699/4f/\"\n",
    "\n",
    "# 0.2 = split training dataset into 20% validation data, 80% training data\n",
    "train_val_split = 0.2\n",
    "\n",
    "# for mask 4AF - acc = 4, cen = 0.08\n",
    "acc = 4\n",
    "cen_fract = 0.08\n",
    "\n",
    "seed = True # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# Model parameters\n",
    "in_chans = 1\n",
    "out_chans = 1\n",
    "chans = 16\n",
    "# This needs to be (1,1) for the model to run...why...\n",
    "kernel_size=(1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 30\n",
    "dropout_prob = 0.01\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "step_size = 11\n",
    "lr_gamma = 0.1 # change in learning rate\n",
    "num_pool_layers = 4\n",
    "\n",
    "# Check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path):\n",
    "#     eventually make this random subsets by shuffling data for\n",
    "    \"\"\" Go through training data path, list all file names, the file paths and the slices of subjects. \n",
    "    Split into training and validation set depending on value of train_val_split\n",
    "    \"\"\"\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    files = len(os.listdir(train_data_path))\n",
    "    train_files_num = (1 - train_val_split) * files\n",
    "\n",
    "    i = 0    \n",
    "    for fname in sorted(os.listdir(train_data_path)):\n",
    "        subject_data_path = os.path.join(train_data_path, fname)\n",
    "        if not os.path.isfile(subject_data_path): continue \n",
    "        \n",
    "        if i <= train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                train_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        elif i > train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                val_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        i += 1\n",
    "        \n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]             \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: \n",
    "        norm = 1e-6\n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Unet: Neural networks with downsampling and upsampling. ref: https://github.com/facebookresearch/fastMRI/blob/master/models/unet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.down_sample_layers = ModuleList([ConvBlock(in_chans, chans, drop_prob, kernel_size)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob, kernel_size)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob, kernel_size)\n",
    "\n",
    "        self.up_sample_layers = ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob, kernel_size)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob, kernel_size)]\n",
    "        self.conv2 = Sequential(\n",
    "            Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_chans, out_chans, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(in_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args: input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns: (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, drop_prob={self.drop_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und)\n",
    "        input = T.center_crop(input, [320, 320])\n",
    "        input = input[None, ...].to(device, dtype=torch.float)\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und) # absolute values\n",
    "        input = T.center_crop(input, [320, 320]) # crop to 320  x 320\n",
    "        input = input[None, ...].to(device, dtype=torch.float) # 3d to 4d tensor\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':       \n",
    "    train_data, val_data  = load_data_path(data_path_train) # first load all file names, paths and slices.\n",
    "\n",
    "    # create data loader for training and validation sets\n",
    "    train_dataset = MRIDataset(train_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "    val_dataset = MRIDataset(val_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    # create model object\n",
    "    model = UnetModel(in_chans=in_chans, out_chans=out_chans, chans=chans, num_pool_layers=4, drop_prob=dropout_prob, kernel_size=kernel_size).to(device)\n",
    "    # use RMSprop as optimizer\n",
    "    optimizer = RMSprop(model.parameters(), learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 16, 320, 320]              32\n",
      "    InstanceNorm2d-2          [1, 16, 320, 320]               0\n",
      "              ReLU-3          [1, 16, 320, 320]               0\n",
      "         Dropout2d-4          [1, 16, 320, 320]               0\n",
      "            Conv2d-5          [1, 16, 320, 320]             272\n",
      "    InstanceNorm2d-6          [1, 16, 320, 320]               0\n",
      "              ReLU-7          [1, 16, 320, 320]               0\n",
      "         Dropout2d-8          [1, 16, 320, 320]               0\n",
      "         ConvBlock-9          [1, 16, 320, 320]               0\n",
      "           Conv2d-10          [1, 32, 160, 160]             544\n",
      "   InstanceNorm2d-11          [1, 32, 160, 160]               0\n",
      "             ReLU-12          [1, 32, 160, 160]               0\n",
      "        Dropout2d-13          [1, 32, 160, 160]               0\n",
      "           Conv2d-14          [1, 32, 160, 160]           1,056\n",
      "   InstanceNorm2d-15          [1, 32, 160, 160]               0\n",
      "             ReLU-16          [1, 32, 160, 160]               0\n",
      "        Dropout2d-17          [1, 32, 160, 160]               0\n",
      "        ConvBlock-18          [1, 32, 160, 160]               0\n",
      "           Conv2d-19            [1, 64, 80, 80]           2,112\n",
      "   InstanceNorm2d-20            [1, 64, 80, 80]               0\n",
      "             ReLU-21            [1, 64, 80, 80]               0\n",
      "        Dropout2d-22            [1, 64, 80, 80]               0\n",
      "           Conv2d-23            [1, 64, 80, 80]           4,160\n",
      "   InstanceNorm2d-24            [1, 64, 80, 80]               0\n",
      "             ReLU-25            [1, 64, 80, 80]               0\n",
      "        Dropout2d-26            [1, 64, 80, 80]               0\n",
      "        ConvBlock-27            [1, 64, 80, 80]               0\n",
      "           Conv2d-28           [1, 128, 40, 40]           8,320\n",
      "   InstanceNorm2d-29           [1, 128, 40, 40]               0\n",
      "             ReLU-30           [1, 128, 40, 40]               0\n",
      "        Dropout2d-31           [1, 128, 40, 40]               0\n",
      "           Conv2d-32           [1, 128, 40, 40]          16,512\n",
      "   InstanceNorm2d-33           [1, 128, 40, 40]               0\n",
      "             ReLU-34           [1, 128, 40, 40]               0\n",
      "        Dropout2d-35           [1, 128, 40, 40]               0\n",
      "        ConvBlock-36           [1, 128, 40, 40]               0\n",
      "           Conv2d-37           [1, 128, 20, 20]          16,512\n",
      "   InstanceNorm2d-38           [1, 128, 20, 20]               0\n",
      "             ReLU-39           [1, 128, 20, 20]               0\n",
      "        Dropout2d-40           [1, 128, 20, 20]               0\n",
      "           Conv2d-41           [1, 128, 20, 20]          16,512\n",
      "   InstanceNorm2d-42           [1, 128, 20, 20]               0\n",
      "             ReLU-43           [1, 128, 20, 20]               0\n",
      "        Dropout2d-44           [1, 128, 20, 20]               0\n",
      "        ConvBlock-45           [1, 128, 20, 20]               0\n",
      "           Conv2d-46            [1, 64, 40, 40]          16,448\n",
      "   InstanceNorm2d-47            [1, 64, 40, 40]               0\n",
      "             ReLU-48            [1, 64, 40, 40]               0\n",
      "        Dropout2d-49            [1, 64, 40, 40]               0\n",
      "           Conv2d-50            [1, 64, 40, 40]           4,160\n",
      "   InstanceNorm2d-51            [1, 64, 40, 40]               0\n",
      "             ReLU-52            [1, 64, 40, 40]               0\n",
      "        Dropout2d-53            [1, 64, 40, 40]               0\n",
      "        ConvBlock-54            [1, 64, 40, 40]               0\n",
      "           Conv2d-55            [1, 32, 80, 80]           4,128\n",
      "   InstanceNorm2d-56            [1, 32, 80, 80]               0\n",
      "             ReLU-57            [1, 32, 80, 80]               0\n",
      "        Dropout2d-58            [1, 32, 80, 80]               0\n",
      "           Conv2d-59            [1, 32, 80, 80]           1,056\n",
      "   InstanceNorm2d-60            [1, 32, 80, 80]               0\n",
      "             ReLU-61            [1, 32, 80, 80]               0\n",
      "        Dropout2d-62            [1, 32, 80, 80]               0\n",
      "        ConvBlock-63            [1, 32, 80, 80]               0\n",
      "           Conv2d-64          [1, 16, 160, 160]           1,040\n",
      "   InstanceNorm2d-65          [1, 16, 160, 160]               0\n",
      "             ReLU-66          [1, 16, 160, 160]               0\n",
      "        Dropout2d-67          [1, 16, 160, 160]               0\n",
      "           Conv2d-68          [1, 16, 160, 160]             272\n",
      "   InstanceNorm2d-69          [1, 16, 160, 160]               0\n",
      "             ReLU-70          [1, 16, 160, 160]               0\n",
      "        Dropout2d-71          [1, 16, 160, 160]               0\n",
      "        ConvBlock-72          [1, 16, 160, 160]               0\n",
      "           Conv2d-73          [1, 16, 320, 320]             528\n",
      "   InstanceNorm2d-74          [1, 16, 320, 320]               0\n",
      "             ReLU-75          [1, 16, 320, 320]               0\n",
      "        Dropout2d-76          [1, 16, 320, 320]               0\n",
      "           Conv2d-77          [1, 16, 320, 320]             272\n",
      "   InstanceNorm2d-78          [1, 16, 320, 320]               0\n",
      "             ReLU-79          [1, 16, 320, 320]               0\n",
      "        Dropout2d-80          [1, 16, 320, 320]               0\n",
      "        ConvBlock-81          [1, 16, 320, 320]               0\n",
      "           Conv2d-82           [1, 8, 320, 320]             136\n",
      "           Conv2d-83           [1, 1, 320, 320]               9\n",
      "           Conv2d-84           [1, 1, 320, 320]               2\n",
      "================================================================\n",
      "Total params: 94,083\n",
      "Trainable params: 94,083\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 383.98\n",
      "Params size (MB): 0.36\n",
      "Estimated Total Size (MB): 384.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_size=(channels, H, W)\n",
    "summary(model, input_size=(1, 320, 320), batch_size=1, device=str(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1714\n",
      "Validating on 420\n",
      "Epoch: 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'multiprocessing.util' has no attribute '_flush_std_streams'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-68e606395eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loss_ot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-90003c86b8f0>\u001b[0m in \u001b[0;36mtraining_epoch\u001b[0;34m(epoch, model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# img ground truth, img undersampled, raw data understampled, masks, norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawdata_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/rh-python36/root/usr/lib64/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'multiprocessing.util' has no attribute '_flush_std_streams'"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "scheduler = StepLR(optimizer, step_size, lr_gamma)\n",
    "current_epoch = 0\n",
    "# record loss overtime for plotting\n",
    "train_loss_ot = []\n",
    "val_loss_ot = []\n",
    "\n",
    "print(\"Training on \" + str(len(train_data)))\n",
    "print(\"Validating on \" + str(len(val_data)))\n",
    "    \n",
    "# run model epochs\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    scheduler.step(epoch)\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(epochs))\n",
    "    train_loss, train_time = training_epoch(epoch, model, train_loader, optimizer)\n",
    "    val_loss, val_time = validation_epoch(epoch, model, val_loader, optimizer)\n",
    "    train_loss_ot.append(train_loss)\n",
    "    val_loss_ot.append(val_loss)\n",
    "    print(\" Train Loss: \" + str(train_loss) + \" | Validation Loss: \" + str(val_loss))\n",
    "    print(\"Train Time: \" + str(train_time) + \" | Validation Time: \" + str(val_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_ot, label='train')\n",
    "plt.plot(val_loss_ot, label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss-4f-' + str(epochs) + str(chans) + 'rmsprop.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model (this method is just for testing visualization of the images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = next(iter(train_loader))\n",
    "# img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "pred = model(img_und_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt = T.complex_abs(img_gt)\n",
    "img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "img_gt_2d = img_gt_cropped[-1,:,:]\n",
    "\n",
    "# img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320,320])\n",
    "# to 2s\n",
    "img_und_2d = img_und_cropped[-1,:,:]\n",
    "\n",
    "# bring to cpu\n",
    "predc = pred.cpu().detach()\n",
    "# prediction 4d -> 3d\n",
    "pred3d = predc[-1,:,:,:]\n",
    "# prediction 3d -> 2d\n",
    "pred2d = predc[-1,-1,:,:]\n",
    "\n",
    "print(img_gt_2d.shape)\n",
    "print(img_und_2d.shape)\n",
    "print(pred2d.shape)\n",
    "all_imgs = torch.stack([img_und_2d,img_gt_2d, pred2d], dim=0)\n",
    "show_slices(all_imgs, [0,1,2], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results from training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "preds = []\n",
    "\n",
    "for iter, data_sample in enumerate(train_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "    img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "    \n",
    "    pred = model(img_und_normalised)\n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())\n",
    "\n",
    "for iter, data_sample in enumerate(val_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "    \n",
    "    img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on training data\n",
    "We can evaluate SSIM on the whole volume in the region of interset (320x320 central region) with respect to ground truth. As can be seen, the more aggressive sampling we have, the lower SSIM value we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). Required 3D input np arrays\"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1,2,0), pred.transpose(1,2,0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(gts)\n",
    "i = 0\n",
    "ssim_comb = 0\n",
    "for i in range(0,length):\n",
    "    ssim_comb += ssim(gts[i], preds[i])\n",
    "\n",
    "ssim = ssim_comb / length\n",
    "\n",
    "print(\"Average SSIM: \" + str(ssim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run against Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRITestDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_test_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batch(subject_id, acc, center_fract, use_seed):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace_4af'][slice]             \n",
    "        slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "        S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    img_und = T.ifft2(slice_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: \n",
    "        norm = 1e-6\n",
    "    # normalized data\n",
    "    img_und, rawdata_und = img_und/norm, slice_kspace/norm\n",
    "        \n",
    "    return img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(reconstruction, fname, key):\n",
    "    \"\"\"\n",
    "    Saves the reconstructions from a model into h5 files that is appropriate for submission\n",
    "    to the leaderboard.\n",
    "    \"\"\"\n",
    "    if not (os.path.exists(out_dir)): \n",
    "        os.makedirs(out_dir)\n",
    "    subject_path = os.path.join(out_dir, fname)\n",
    "    print(subject_path)\n",
    "    \n",
    "    if (os.file.exists(subject_path)):\n",
    "        with h5py.File(subject_path, 'a') as f:\n",
    "            f.create_dataset(key, data=reconstruction)\n",
    "    else:\n",
    "        with h5py.File(subject_path, 'w') as f:\n",
    "            f.create_dataset(key, data=reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_data = []\n",
    "key = 'kspace_4af'\n",
    "for fname in sorted(os.listdir(data_path_test)):\n",
    "    subject_path = os.path.join(data_path_test, fname)\n",
    "    if not os.path.isfile(subject_path): continue \n",
    "        \n",
    "    with h5py.File(subject_path,  \"r\") as hf:\n",
    "        num_slice_4f = hf[key].shape[0]\n",
    "        mask_4f = hf['mask_4af']\n",
    "\n",
    "        test_data += [(fname, subject_path, slice) for slice in range(5, num_slice_4f)]\n",
    "        # create data loader \n",
    "        test_dataset = MRITestDataset(test_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "        test_preds = []\n",
    "\n",
    "        print(len(test_loader))\n",
    "        for iter, data_sample in enumerate(test_loader):\n",
    "            # img_und , raw data undersampled, mask, norm\n",
    "            volume_kspace_4af, rawdata_und, mask_4af, norm = data_sample\n",
    "            volume_image_abs = T.complex_abs(volume_kspace_4af)   # Compute absolute value to get a real image\n",
    "\n",
    "            volume_image_cropped = T.center_crop(volume_image_abs, [320, 320])    \n",
    "            volume_image_padded = volume_image_cropped[None, ...].to(device, dtype=torch.float)\n",
    "            volume_image_normalised = volume_image_padded * norm.to(device, dtype=torch.float) \n",
    "\n",
    "            pred = model(volume_image_normalised)\n",
    "\n",
    "            # bring to cpu\n",
    "            predc = pred.cpu().detach()\n",
    "            # prediction 4d -> 3d\n",
    "            pred3d = predc[-1,:,:,:]\n",
    "            # prediction 3d -> 2d\n",
    "            pred2d = predc[-1,-1,:,:]\n",
    "            test_preds.append(pred2d)\n",
    "\n",
    "        predvol = torch.stack(test_preds, dim=0)\n",
    "        show_slices(predvol, [0,1], cmap='gray')\n",
    "        plt.pause(1)\n",
    "        save_reconstructions(predvol, fname, key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
