{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "(150 - 200 words)\n",
    "\n",
    "The long acquisition time in fully sampled MRI leads to low patient throughput, problems with patient comfort and compliance, artefacts from patient motion, and high exam costs. Reducing acquisition time by under-sampling helps to mitigate these issues, but at the cost of reconstructed image quality. The aim of this machine learning task is to improve the viability of the more efficient under-sampling strategy by designing a system that maximises the quality of under sampled reconstructions.\n",
    "\n",
    "The MRI dataset provided contained raw k-space data from 100 3D volumes, each having approximately 30-40 2D slices. The data was split into training and testing sets with ratio 7:3 respectively. The training set contained only fully sampled k-space data from which ground truth and under-sampled data could be derived. The test set contained 4-fold and 8-fold under sampled k-space data as well as the 4-fold and 8-fold masks that generated this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "(450 - 600 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some through research of various types of neural networks, we decided that the best model for this project would be a U-Net. This is because, at the time of writing, U-Nets are regarded to be one of the best types of Convolutional Neural Networks for biomedical imaging.\n",
    "\n",
    "The major attraction to U-Nets for such tasks is their apparent lack of overfitting risk.\n",
    "\n",
    "[Ref 1] The performance of the U-Net models continues to increase with increasing model capacity, and 17 even the largest model with over 200 million parameters is unable to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other types of Neural Networks that we looked at for this task were: \n",
    "- Convolutional Neural Network\n",
    "- Recurrent Neural Network\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to experiment with the following hyperparameters (change as necessary):\n",
    "- Epochs\n",
    "- Learning Rate\n",
    "- Dropout Probability\n",
    "- Step Size\n",
    "- Batch Size\n",
    "- Pooling Layers\n",
    "\n",
    "Guess we should explain what each are and why we c\n",
    "In addition to this we altered the layers in the neural network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "(600 - 800 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we began implementing a U-Net from scratch however, quickly ran into problems that were quite ambiguous. After some research, we found a preexisting model provided in the facebook research fast MRI project (https://github.com/facebookresearch/fastMRI/tree/master/models/unet). \n",
    "\n",
    "By opting to use a preexisting model, we could ensure that our basic model configuration was correct. It also allowed us to deduce that any issues along the way were caused by parts of the code that were not directly related to the model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial implementation of the neural network had the following structure. The model carries out down-sampling and up-sampling, resulting in the formation of two deep convolutional networks within the U-Net."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Conv2d-1           [1, 8, 320, 320]              16\n",
    "    InstanceNorm2d-2           [1, 8, 320, 320]               0\n",
    "              ReLU-3           [1, 8, 320, 320]               0\n",
    "         Dropout2d-4           [1, 8, 320, 320]               0\n",
    "            Conv2d-5           [1, 8, 320, 320]              72\n",
    "    InstanceNorm2d-6           [1, 8, 320, 320]               0\n",
    "              ReLU-7           [1, 8, 320, 320]               0\n",
    "         Dropout2d-8           [1, 8, 320, 320]               0\n",
    "         ConvBlock-9           [1, 8, 320, 320]               0\n",
    "           Conv2d-10          [1, 16, 160, 160]             144\n",
    "   InstanceNorm2d-11          [1, 16, 160, 160]               0\n",
    "             ReLU-12          [1, 16, 160, 160]               0\n",
    "        Dropout2d-13          [1, 16, 160, 160]               0\n",
    "           Conv2d-14          [1, 16, 160, 160]             272\n",
    "   InstanceNorm2d-15          [1, 16, 160, 160]               0\n",
    "             ReLU-16          [1, 16, 160, 160]               0\n",
    "        Dropout2d-17          [1, 16, 160, 160]               0\n",
    "        ConvBlock-18          [1, 16, 160, 160]               0\n",
    "           Conv2d-19            [1, 32, 80, 80]             544\n",
    "   InstanceNorm2d-20            [1, 32, 80, 80]               0\n",
    "             ReLU-21            [1, 32, 80, 80]               0\n",
    "        Dropout2d-22            [1, 32, 80, 80]               0\n",
    "           Conv2d-23            [1, 32, 80, 80]           1,056\n",
    "   InstanceNorm2d-24            [1, 32, 80, 80]               0\n",
    "             ReLU-25            [1, 32, 80, 80]               0\n",
    "        Dropout2d-26            [1, 32, 80, 80]               0\n",
    "        ConvBlock-27            [1, 32, 80, 80]               0\n",
    "           Conv2d-28            [1, 64, 40, 40]           2,112\n",
    "   InstanceNorm2d-29            [1, 64, 40, 40]               0\n",
    "             ReLU-30            [1, 64, 40, 40]               0\n",
    "        Dropout2d-31            [1, 64, 40, 40]               0\n",
    "           Conv2d-32            [1, 64, 40, 40]           4,160\n",
    "   InstanceNorm2d-33            [1, 64, 40, 40]               0\n",
    "             ReLU-34            [1, 64, 40, 40]               0\n",
    "        Dropout2d-35            [1, 64, 40, 40]               0\n",
    "        ConvBlock-36            [1, 64, 40, 40]               0\n",
    "           Conv2d-37            [1, 64, 20, 20]           4,160\n",
    "   InstanceNorm2d-38            [1, 64, 20, 20]               0\n",
    "             ReLU-39            [1, 64, 20, 20]               0\n",
    "        Dropout2d-40            [1, 64, 20, 20]               0\n",
    "           Conv2d-41            [1, 64, 20, 20]           4,160\n",
    "   InstanceNorm2d-42            [1, 64, 20, 20]               0\n",
    "             ReLU-43            [1, 64, 20, 20]               0\n",
    "        Dropout2d-44            [1, 64, 20, 20]               0\n",
    "        ConvBlock-45            [1, 64, 20, 20]               0\n",
    "           Conv2d-46            [1, 32, 40, 40]           4,128\n",
    "   InstanceNorm2d-47            [1, 32, 40, 40]               0\n",
    "             ReLU-48            [1, 32, 40, 40]               0\n",
    "        Dropout2d-49            [1, 32, 40, 40]               0\n",
    "           Conv2d-50            [1, 32, 40, 40]           1,056\n",
    "   InstanceNorm2d-51            [1, 32, 40, 40]               0\n",
    "             ReLU-52            [1, 32, 40, 40]               0\n",
    "        Dropout2d-53            [1, 32, 40, 40]               0\n",
    "        ConvBlock-54            [1, 32, 40, 40]               0\n",
    "           Conv2d-55            [1, 16, 80, 80]           1,040\n",
    "   InstanceNorm2d-56            [1, 16, 80, 80]               0\n",
    "             ReLU-57            [1, 16, 80, 80]               0\n",
    "        Dropout2d-58            [1, 16, 80, 80]               0\n",
    "           Conv2d-59            [1, 16, 80, 80]             272\n",
    "   InstanceNorm2d-60            [1, 16, 80, 80]               0\n",
    "             ReLU-61            [1, 16, 80, 80]               0\n",
    "        Dropout2d-62            [1, 16, 80, 80]               0\n",
    "        ConvBlock-63            [1, 16, 80, 80]               0\n",
    "           Conv2d-64           [1, 8, 160, 160]             264\n",
    "   InstanceNorm2d-65           [1, 8, 160, 160]               0\n",
    "             ReLU-66           [1, 8, 160, 160]               0\n",
    "        Dropout2d-67           [1, 8, 160, 160]               0\n",
    "           Conv2d-68           [1, 8, 160, 160]              72\n",
    "   InstanceNorm2d-69           [1, 8, 160, 160]               0\n",
    "             ReLU-70           [1, 8, 160, 160]               0\n",
    "        Dropout2d-71           [1, 8, 160, 160]               0\n",
    "        ConvBlock-72           [1, 8, 160, 160]               0\n",
    "           Conv2d-73           [1, 8, 320, 320]             136\n",
    "   InstanceNorm2d-74           [1, 8, 320, 320]               0\n",
    "             ReLU-75           [1, 8, 320, 320]               0\n",
    "        Dropout2d-76           [1, 8, 320, 320]               0\n",
    "           Conv2d-77           [1, 8, 320, 320]              72\n",
    "   InstanceNorm2d-78           [1, 8, 320, 320]               0\n",
    "             ReLU-79           [1, 8, 320, 320]               0\n",
    "        Dropout2d-80           [1, 8, 320, 320]               0\n",
    "        ConvBlock-81           [1, 8, 320, 320]               0\n",
    "           Conv2d-82           [1, 4, 320, 320]              36\n",
    "           Conv2d-83           [1, 1, 320, 320]               5\n",
    "           Conv2d-84           [1, 1, 320, 320]               2\n",
    "================================================================\n",
    "Total params: 23,779\n",
    "Trainable params: 23,779\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.39\n",
    "Forward/backward pass size (MB): 192.77\n",
    "Params size (MB): 0.09\n",
    "Estimated Total Size (MB): 193.25\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer we chose to use was ReLU (Rectified Linear Unit)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An imperative part of the task was to monitor for signs of overfitting, by splitting training data into training and validation data. Overfitting occurs when the model is learning the training data too well, causing the training loss to continuously decrease, while the validation loss continuously increases.\n",
    "\n",
    "The mechanism for splitting the data is rather naive, as it takes the first x images as training data, and the remaining 1-x as validation data. A future enhancement to this would be to randomise the selection process, as this could boost model accuracy, due to structural difference in images towards the end of the dataset.\n",
    "\n",
    "This allows for each epoch to run a 'training epoch' and a 'validation epoch', as shown in the code snippet below.\n",
    "\n",
    "As epochs progress, we monitor the training and validation loss, storing values into an array. Doing this allows us to plot a graph of the training/validation loss over time, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Epochs for training\n",
    "# StepLR sets the learning rate of each parameter group to the initial lr decayed by gamma every step_size epochs.\n",
    "scheduler = StepLR(optimizer, step_size, lr_gamma) \n",
    "current_epoch = 0\n",
    "# record loss overtime for plotting\n",
    "train_loss_ot = []\n",
    "val_loss_ot = []\n",
    "\n",
    "print(\"Training on \" + str(len(train_data)))\n",
    "print(\"Validating on \" + str(len(val_data)))\n",
    "    \n",
    "# run model epochs\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    scheduler.step(epoch)\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(epochs))\n",
    "    train_loss, train_time = training_epoch(epoch, model, train_loader, optimizer) # run a training epoch\n",
    "    val_loss, val_time = validation_epoch(epoch, model, val_loader, optimizer) # run a validation epoch\n",
    "    train_loss_ot.append(train_loss)\n",
    "    val_loss_ot.append(val_loss)\n",
    "    print(\" Train Loss: \" + str(train_loss) + \" | Validation Loss: \" + str(val_loss)) # print loss for the completeted epoch\n",
    "    print(\"Train Time: \" + str(train_time) + \" | Validation Time: \" + str(val_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### insert image of some graphs here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure performance, we used training and validation loss, as discussed above, as well as SSIM.\n",
    "\n",
    "SSIM (Structural Similarity Index Measure), is a measurement of how structurally similar two images are, making it a form of accuracy measure. This allows for a calculation to be made regarding the average 'accuracy' of the model's generated images, and their corresponding ground truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). Required 3D input np arrays\"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1,2,0), pred.transpose(1,2,0), multichannel=True, data_range=gt.max()\n",
    "    )  \n",
    "\n",
    "# calculate average ssim for training and validation data\n",
    "length = len(gts)\n",
    "i = 0\n",
    "ssim_comb = 0\n",
    "for i in range(0,length):\n",
    "    ssim_comb += ssim(gts[i], preds[i])\n",
    "\n",
    "ssim = ssim_comb / length\n",
    "\n",
    "print(\"Average SSIM: \" + str(ssim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "(1350 - 1800 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is just an example of the kind of layout and stuff to put in this section...we'll need to choose the most worthwhile experimentations to talk about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting With Hyperparmeters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Epochs  | Learning Rate | Dropout Probability  | Step Size | Training Loss  | Validation Loss | SSIM |\n",
    "|---|---|---|---|---|---|---|\n",
    "|30 |0.01|0.001|15|0.0488 |0.0448|0.41407|\n",
    "|20 |0.1|0.001|15|0.056 |0.054 |0.41165|\n",
    "|20 |0.1|0.01 |15|0.0685|0.0642|0.42711|\n",
    "|50 |0.1|0.01 |15|0.0659|0.0641|0.41018|\n",
    "|75 |0.1|0.01 |15|0.0648|0.0622|0.42522|\n",
    "|20 |0.1|0.01 |25|0.1144|0.1141|0.38437|\n",
    "|75 |0.1|0.01 |25|0.0581|0.0549|0.46331|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noticable from the table above, a low training/validation loss does not necessarily mean the average SSIM of the images will be higher..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Conclusions\n",
    "(300 - 400 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Contribution\n",
    "(150 - 200 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://arxiv.org/pdf/1811.08839.pdf\n",
    "2. https://arxiv.org/ftp/arxiv/papers/1704/1704.06825.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
