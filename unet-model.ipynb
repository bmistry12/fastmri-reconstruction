{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment Variables and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "# from unet import ConvBlock, UnetModel\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, Sequential, InstanceNorm2d, ReLU, Dropout2d, Module, ModuleList, functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import compare_ssim \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/data/local/NC2019MRI/train'\n",
    "val_data_path = '/data/local/NC2019MRI/train'\n",
    "test_data_path = '/data/local/NC2019MRI/test'\n",
    "\n",
    "# for mask 4AF - acc = 4, cen = 0.08\n",
    "# for mask 8AF - acc = 8, cen = 0.04\n",
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "seed = True # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# Model parameters\n",
    "in_chans = 1\n",
    "out_chans = 1\n",
    "chans = 8\n",
    "# This needs to be (1,1) for the model to run...why...\n",
    "kernel_size=(1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10\n",
    "dropout_prob = 0.001\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "step_size = 15\n",
    "lr_gamma = 0.1 # change in learning rate\n",
    "num_pool_layers = 3\n",
    "\n",
    "# Check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "        data_list[train_and_val[i]] = []\n",
    "        which_data_path = data_path[i]\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path)):\n",
    "            subject_data_path = os.path.join(which_data_path, fname)\n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "        \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]             \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: \n",
    "        norm = 1e-6\n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Unet: Neural networks with downsampling and upsampling. ref: https://github.com/facebookresearch/fastMRI/blob/master/models/unet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.down_sample_layers = ModuleList([ConvBlock(in_chans, chans, drop_prob, kernel_size)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob, kernel_size)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob, kernel_size)\n",
    "\n",
    "        self.up_sample_layers = ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob, kernel_size)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob, kernel_size)]\n",
    "        self.conv2 = Sequential(\n",
    "            Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_chans, out_chans, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(in_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args: input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns: (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, drop_prob={self.drop_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und)\n",
    "        input = T.center_crop(input, [320, 320])\n",
    "        input = input[None, ...].to(device, dtype=torch.float)\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        \n",
    "#         if iter % report_interval == 0:\n",
    "#             print('Epoch: ' + str(epoch) + \"/\" + str(epochs)  + \" \\n Iteration\" + str(iter/len(data_loader)) +\n",
    "#                   \" \\n Loss: \" + str(loss.item()) + \"Avg Loss: \" + str(avg_loss) + \n",
    "#                   \" \\n Time: \" + str(time.perf_counter() - start_iter)\n",
    "#             )\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnetModel(\n",
      "  (down_sample_layers): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv): ConvBlock(\n",
      "    (layers): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout2d(p=0.001, inplace=False)\n",
      "      (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout2d(p=0.001, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (up_sample_layers): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ConvBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): ReLU()\n",
      "        (3): Dropout2d(p=0.001, inplace=False)\n",
      "        (4): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (5): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (6): ReLU()\n",
      "        (7): Dropout2d(p=0.001, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = '/data/local/NC2019MRI/train'\n",
    "    data_path_val = '/data/local/NC2019MRI/train'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    train_data = data_list['train']\n",
    "    val_data = data_list['val']\n",
    "    \n",
    "    # create data loader for training and validation sets\n",
    "    train_dataset = MRIDataset(train_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "    val_dataset = MRIDataset(val_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    # create model object\n",
    "    model = UnetModel(in_chans=in_chans, out_chans=out_chans, chans=chans, num_pool_layers=4, drop_prob=dropout_prob, kernel_size=kernel_size).to(device)\n",
    "    # use RMSprop as optimizer\n",
    "    optimizer = RMSprop(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    print(model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 8, 320, 320]              16\n",
      "    InstanceNorm2d-2           [1, 8, 320, 320]               0\n",
      "              ReLU-3           [1, 8, 320, 320]               0\n",
      "         Dropout2d-4           [1, 8, 320, 320]               0\n",
      "            Conv2d-5           [1, 8, 320, 320]              72\n",
      "    InstanceNorm2d-6           [1, 8, 320, 320]               0\n",
      "              ReLU-7           [1, 8, 320, 320]               0\n",
      "         Dropout2d-8           [1, 8, 320, 320]               0\n",
      "         ConvBlock-9           [1, 8, 320, 320]               0\n",
      "           Conv2d-10          [1, 16, 160, 160]             144\n",
      "   InstanceNorm2d-11          [1, 16, 160, 160]               0\n",
      "             ReLU-12          [1, 16, 160, 160]               0\n",
      "        Dropout2d-13          [1, 16, 160, 160]               0\n",
      "           Conv2d-14          [1, 16, 160, 160]             272\n",
      "   InstanceNorm2d-15          [1, 16, 160, 160]               0\n",
      "             ReLU-16          [1, 16, 160, 160]               0\n",
      "        Dropout2d-17          [1, 16, 160, 160]               0\n",
      "        ConvBlock-18          [1, 16, 160, 160]               0\n",
      "           Conv2d-19            [1, 32, 80, 80]             544\n",
      "   InstanceNorm2d-20            [1, 32, 80, 80]               0\n",
      "             ReLU-21            [1, 32, 80, 80]               0\n",
      "        Dropout2d-22            [1, 32, 80, 80]               0\n",
      "           Conv2d-23            [1, 32, 80, 80]           1,056\n",
      "   InstanceNorm2d-24            [1, 32, 80, 80]               0\n",
      "             ReLU-25            [1, 32, 80, 80]               0\n",
      "        Dropout2d-26            [1, 32, 80, 80]               0\n",
      "        ConvBlock-27            [1, 32, 80, 80]               0\n",
      "           Conv2d-28            [1, 64, 40, 40]           2,112\n",
      "   InstanceNorm2d-29            [1, 64, 40, 40]               0\n",
      "             ReLU-30            [1, 64, 40, 40]               0\n",
      "        Dropout2d-31            [1, 64, 40, 40]               0\n",
      "           Conv2d-32            [1, 64, 40, 40]           4,160\n",
      "   InstanceNorm2d-33            [1, 64, 40, 40]               0\n",
      "             ReLU-34            [1, 64, 40, 40]               0\n",
      "        Dropout2d-35            [1, 64, 40, 40]               0\n",
      "        ConvBlock-36            [1, 64, 40, 40]               0\n",
      "           Conv2d-37            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-38            [1, 64, 20, 20]               0\n",
      "             ReLU-39            [1, 64, 20, 20]               0\n",
      "        Dropout2d-40            [1, 64, 20, 20]               0\n",
      "           Conv2d-41            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-42            [1, 64, 20, 20]               0\n",
      "             ReLU-43            [1, 64, 20, 20]               0\n",
      "        Dropout2d-44            [1, 64, 20, 20]               0\n",
      "        ConvBlock-45            [1, 64, 20, 20]               0\n",
      "           Conv2d-46            [1, 32, 40, 40]           4,128\n",
      "   InstanceNorm2d-47            [1, 32, 40, 40]               0\n",
      "             ReLU-48            [1, 32, 40, 40]               0\n",
      "        Dropout2d-49            [1, 32, 40, 40]               0\n",
      "           Conv2d-50            [1, 32, 40, 40]           1,056\n",
      "   InstanceNorm2d-51            [1, 32, 40, 40]               0\n",
      "             ReLU-52            [1, 32, 40, 40]               0\n",
      "        Dropout2d-53            [1, 32, 40, 40]               0\n",
      "        ConvBlock-54            [1, 32, 40, 40]               0\n",
      "           Conv2d-55            [1, 16, 80, 80]           1,040\n",
      "   InstanceNorm2d-56            [1, 16, 80, 80]               0\n",
      "             ReLU-57            [1, 16, 80, 80]               0\n",
      "        Dropout2d-58            [1, 16, 80, 80]               0\n",
      "           Conv2d-59            [1, 16, 80, 80]             272\n",
      "   InstanceNorm2d-60            [1, 16, 80, 80]               0\n",
      "             ReLU-61            [1, 16, 80, 80]               0\n",
      "        Dropout2d-62            [1, 16, 80, 80]               0\n",
      "        ConvBlock-63            [1, 16, 80, 80]               0\n",
      "           Conv2d-64           [1, 8, 160, 160]             264\n",
      "   InstanceNorm2d-65           [1, 8, 160, 160]               0\n",
      "             ReLU-66           [1, 8, 160, 160]               0\n",
      "        Dropout2d-67           [1, 8, 160, 160]               0\n",
      "           Conv2d-68           [1, 8, 160, 160]              72\n",
      "   InstanceNorm2d-69           [1, 8, 160, 160]               0\n",
      "             ReLU-70           [1, 8, 160, 160]               0\n",
      "        Dropout2d-71           [1, 8, 160, 160]               0\n",
      "        ConvBlock-72           [1, 8, 160, 160]               0\n",
      "           Conv2d-73           [1, 8, 320, 320]             136\n",
      "   InstanceNorm2d-74           [1, 8, 320, 320]               0\n",
      "             ReLU-75           [1, 8, 320, 320]               0\n",
      "        Dropout2d-76           [1, 8, 320, 320]               0\n",
      "           Conv2d-77           [1, 8, 320, 320]              72\n",
      "   InstanceNorm2d-78           [1, 8, 320, 320]               0\n",
      "             ReLU-79           [1, 8, 320, 320]               0\n",
      "        Dropout2d-80           [1, 8, 320, 320]               0\n",
      "        ConvBlock-81           [1, 8, 320, 320]               0\n",
      "           Conv2d-82           [1, 4, 320, 320]              36\n",
      "           Conv2d-83           [1, 1, 320, 320]               5\n",
      "           Conv2d-84           [1, 1, 320, 320]               2\n",
      "================================================================\n",
      "Total params: 23,779\n",
      "Trainable params: 23,779\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 192.77\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 193.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_size=(channels, H, W)\n",
    "summary(model, input_size=(1, 320, 320), batch_size=1, device=str(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Train Loss: 0.0743443809944086\n",
      " Train Time: 54.03494455292821\n",
      "Epoch: 2/10\n",
      "Train Loss: 0.07216381901296785\n",
      " Train Time: 54.463504210929386\n",
      "Epoch: 3/10\n",
      "Train Loss: 0.0742423875625277\n",
      " Train Time: 54.39691932790447\n",
      "Epoch: 4/10\n",
      "Train Loss: 0.07326458510242895\n",
      " Train Time: 54.66777957894374\n",
      "Epoch: 5/10\n",
      "Train Loss: 0.07115037995438933\n",
      " Train Time: 54.370545980054885\n",
      "Epoch: 6/10\n",
      "Train Loss: 0.07134568517070149\n",
      " Train Time: 54.79034864902496\n",
      "Epoch: 7/10\n",
      "Train Loss: 0.06977770871890067\n",
      " Train Time: 54.90725540393032\n",
      "Epoch: 8/10\n",
      "Train Loss: 0.07205330752872903\n",
      " Train Time: 54.63538568804506\n",
      "Epoch: 9/10\n",
      "Train Loss: 0.06878698024132916\n",
      " Train Time: 54.46297730098013\n",
      "Epoch: 10/10\n",
      "Train Loss: 0.07043179508218278\n",
      " Train Time: 54.49156376591418\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "scheduler = StepLR(optimizer, step_size, lr_gamma)\n",
    "current_epoch = 0\n",
    "\n",
    "# run model epochs\n",
    "report_interval = 100\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    scheduler.step(epoch)\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(epochs))\n",
    "    train_loss, train_time = training_epoch(epoch, model, train_loader, optimizer)\n",
    "\n",
    "    print(\" Train Loss: \" + str(train_loss) + \"\\n Train Time: \" + str(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "We can evaluate SSIM on the whole volume in the region of interset (320x320 central region) with respect to ground truth. As can be seen, the more aggressive sampling we have, the lower SSIM value we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0371, 0.0357, 0.0350,  ..., 0.0463, 0.0442, 0.0464],\n",
      "          [0.0374, 0.0390, 0.0363,  ..., 0.0450, 0.0431, 0.0425],\n",
      "          [0.0370, 0.0366, 0.0367,  ..., 0.0430, 0.0465, 0.0448],\n",
      "          ...,\n",
      "          [0.0352, 0.0350, 0.0365,  ..., 0.3941, 0.3458, 0.3057],\n",
      "          [0.0339, 0.0343, 0.0381,  ..., 0.3913, 0.3498, 0.2993],\n",
      "          [0.0356, 0.0378, 0.0369,  ..., 0.4056, 0.3520, 0.3014]]]],\n",
      "       device='cuda:0', grad_fn=<CudnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "gt, und, rawdata, mask, n = iter(train_loader).next()\n",
    "test = T.complex_abs(und)\n",
    "test = T.center_crop(test, [320, 320])\n",
    "test = test[None, ...].to(device, dtype=torch.float)\n",
    "output = model(test)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-fc21ba7833fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvolume_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mifft2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# Apply Inverse Fourier Transform to get the complex image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvolume_image_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvolume_image\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Compute absolute value to get a real image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/neural_comp_report/functions/transforms.py\u001b[0m in \u001b[0;36mifft2\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mIFFT\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifftshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mifft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = output.to('cpu')\n",
    "print(test.size(-1)) # needs to be 2 for ifft2 to work\n",
    "volume_image = T.ifft2(test)            # Apply Inverse Fourier Transform to get the complex image\n",
    "volume_image_abs = T.complex_abs(volume_image)   # Compute absolute value to get a real image\n",
    "\n",
    "show_slices(volume_image_abs, [5, 10, 20, 30], cmap='gray') # Original images without undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
