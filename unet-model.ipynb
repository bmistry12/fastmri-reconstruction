{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment Variables and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "# from unet import ConvBlock, UnetModel\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, Sequential, InstanceNorm2d, ReLU, Dropout2d, Module, ModuleList, functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import compare_ssim \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs\n",
    "data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_test = '/data/local/NC2019MRI/test'\n",
    "# windows - change as req\n",
    "# data_path_train = r'../NC2019MRI/train'\n",
    "# data_path_test = r'../NC2019MRI/test'\n",
    "\n",
    "# 0.2 = split training dataset into 20% validation data, 80% training data\n",
    "train_val_split = 0.2\n",
    "\n",
    "# for mask 4AF - acc = 4, cen = 0.08\n",
    "# acc = 4\n",
    "# cen_fract = 0.08\n",
    "# for mask 8AF - acc = 8, cen = 0.04\n",
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "seed = True # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# Model parameters\n",
    "in_chans = 1\n",
    "out_chans = 1\n",
    "chans = 8\n",
    "# This needs to be (1,1) for the model to run...why...\n",
    "kernel_size=(1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "dropout_prob = 0.001\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "step_size = 15\n",
    "lr_gamma = 0.1 # change in learning rate\n",
    "num_pool_layers = 3\n",
    "\n",
    "# Check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path):\n",
    "#     eventually make this random subsets by shuffling data for\n",
    "    \"\"\" Go through training data path, list all file names, the file paths and the slices of subjects. \n",
    "    Split into training and validation set depending on value of train_val_split\n",
    "    \"\"\"\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    files = len(os.listdir(train_data_path))\n",
    "    train_files_num = (1 - train_val_split) * files\n",
    "#     val_files_num = train_val_split * files\n",
    "    i = 0    \n",
    "    for fname in sorted(os.listdir(train_data_path)):\n",
    "        subject_data_path = os.path.join(train_data_path, fname)\n",
    "        if not os.path.isfile(subject_data_path): continue \n",
    "        \n",
    "        if i <= train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                train_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        elif i > train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                val_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        i += 1\n",
    "        \n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]             \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: \n",
    "        norm = 1e-6\n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Unet: Neural networks with downsampling and upsampling. ref: https://github.com/facebookresearch/fastMRI/blob/master/models/unet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.down_sample_layers = ModuleList([ConvBlock(in_chans, chans, drop_prob, kernel_size)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob, kernel_size)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob, kernel_size)\n",
    "\n",
    "        self.up_sample_layers = ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob, kernel_size)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob, kernel_size)]\n",
    "        self.conv2 = Sequential(\n",
    "            Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_chans, out_chans, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(in_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args: input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns: (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, drop_prob={self.drop_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und)\n",
    "        input = T.center_crop(input, [320, 320])\n",
    "        input = input[None, ...].to(device, dtype=torch.float)\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und) # absolute values\n",
    "        input = T.center_crop(input, [320, 320]) # crop to 320  x 320\n",
    "        input = input[None, ...].to(device, dtype=torch.float) # 3d to 4d tensor\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':       \n",
    "    train_data, val_data  = load_data_path(data_path_train) # first load all file names, paths and slices.\n",
    "\n",
    "    # create data loader for training and validation sets\n",
    "    train_dataset = MRIDataset(train_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "    val_dataset = MRIDataset(val_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    # create model object\n",
    "    model = UnetModel(in_chans=in_chans, out_chans=out_chans, chans=chans, num_pool_layers=4, drop_prob=dropout_prob, kernel_size=kernel_size).to(device)\n",
    "    # use RMSprop as optimizer\n",
    "    optimizer = RMSprop(model.parameters(), learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 8, 320, 320]              16\n",
      "    InstanceNorm2d-2           [1, 8, 320, 320]               0\n",
      "              ReLU-3           [1, 8, 320, 320]               0\n",
      "         Dropout2d-4           [1, 8, 320, 320]               0\n",
      "            Conv2d-5           [1, 8, 320, 320]              72\n",
      "    InstanceNorm2d-6           [1, 8, 320, 320]               0\n",
      "              ReLU-7           [1, 8, 320, 320]               0\n",
      "         Dropout2d-8           [1, 8, 320, 320]               0\n",
      "         ConvBlock-9           [1, 8, 320, 320]               0\n",
      "           Conv2d-10          [1, 16, 160, 160]             144\n",
      "   InstanceNorm2d-11          [1, 16, 160, 160]               0\n",
      "             ReLU-12          [1, 16, 160, 160]               0\n",
      "        Dropout2d-13          [1, 16, 160, 160]               0\n",
      "           Conv2d-14          [1, 16, 160, 160]             272\n",
      "   InstanceNorm2d-15          [1, 16, 160, 160]               0\n",
      "             ReLU-16          [1, 16, 160, 160]               0\n",
      "        Dropout2d-17          [1, 16, 160, 160]               0\n",
      "        ConvBlock-18          [1, 16, 160, 160]               0\n",
      "           Conv2d-19            [1, 32, 80, 80]             544\n",
      "   InstanceNorm2d-20            [1, 32, 80, 80]               0\n",
      "             ReLU-21            [1, 32, 80, 80]               0\n",
      "        Dropout2d-22            [1, 32, 80, 80]               0\n",
      "           Conv2d-23            [1, 32, 80, 80]           1,056\n",
      "   InstanceNorm2d-24            [1, 32, 80, 80]               0\n",
      "             ReLU-25            [1, 32, 80, 80]               0\n",
      "        Dropout2d-26            [1, 32, 80, 80]               0\n",
      "        ConvBlock-27            [1, 32, 80, 80]               0\n",
      "           Conv2d-28            [1, 64, 40, 40]           2,112\n",
      "   InstanceNorm2d-29            [1, 64, 40, 40]               0\n",
      "             ReLU-30            [1, 64, 40, 40]               0\n",
      "        Dropout2d-31            [1, 64, 40, 40]               0\n",
      "           Conv2d-32            [1, 64, 40, 40]           4,160\n",
      "   InstanceNorm2d-33            [1, 64, 40, 40]               0\n",
      "             ReLU-34            [1, 64, 40, 40]               0\n",
      "        Dropout2d-35            [1, 64, 40, 40]               0\n",
      "        ConvBlock-36            [1, 64, 40, 40]               0\n",
      "           Conv2d-37            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-38            [1, 64, 20, 20]               0\n",
      "             ReLU-39            [1, 64, 20, 20]               0\n",
      "        Dropout2d-40            [1, 64, 20, 20]               0\n",
      "           Conv2d-41            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-42            [1, 64, 20, 20]               0\n",
      "             ReLU-43            [1, 64, 20, 20]               0\n",
      "        Dropout2d-44            [1, 64, 20, 20]               0\n",
      "        ConvBlock-45            [1, 64, 20, 20]               0\n",
      "           Conv2d-46            [1, 32, 40, 40]           4,128\n",
      "   InstanceNorm2d-47            [1, 32, 40, 40]               0\n",
      "             ReLU-48            [1, 32, 40, 40]               0\n",
      "        Dropout2d-49            [1, 32, 40, 40]               0\n",
      "           Conv2d-50            [1, 32, 40, 40]           1,056\n",
      "   InstanceNorm2d-51            [1, 32, 40, 40]               0\n",
      "             ReLU-52            [1, 32, 40, 40]               0\n",
      "        Dropout2d-53            [1, 32, 40, 40]               0\n",
      "        ConvBlock-54            [1, 32, 40, 40]               0\n",
      "           Conv2d-55            [1, 16, 80, 80]           1,040\n",
      "   InstanceNorm2d-56            [1, 16, 80, 80]               0\n",
      "             ReLU-57            [1, 16, 80, 80]               0\n",
      "        Dropout2d-58            [1, 16, 80, 80]               0\n",
      "           Conv2d-59            [1, 16, 80, 80]             272\n",
      "   InstanceNorm2d-60            [1, 16, 80, 80]               0\n",
      "             ReLU-61            [1, 16, 80, 80]               0\n",
      "        Dropout2d-62            [1, 16, 80, 80]               0\n",
      "        ConvBlock-63            [1, 16, 80, 80]               0\n",
      "           Conv2d-64           [1, 8, 160, 160]             264\n",
      "   InstanceNorm2d-65           [1, 8, 160, 160]               0\n",
      "             ReLU-66           [1, 8, 160, 160]               0\n",
      "        Dropout2d-67           [1, 8, 160, 160]               0\n",
      "           Conv2d-68           [1, 8, 160, 160]              72\n",
      "   InstanceNorm2d-69           [1, 8, 160, 160]               0\n",
      "             ReLU-70           [1, 8, 160, 160]               0\n",
      "        Dropout2d-71           [1, 8, 160, 160]               0\n",
      "        ConvBlock-72           [1, 8, 160, 160]               0\n",
      "           Conv2d-73           [1, 8, 320, 320]             136\n",
      "   InstanceNorm2d-74           [1, 8, 320, 320]               0\n",
      "             ReLU-75           [1, 8, 320, 320]               0\n",
      "        Dropout2d-76           [1, 8, 320, 320]               0\n",
      "           Conv2d-77           [1, 8, 320, 320]              72\n",
      "   InstanceNorm2d-78           [1, 8, 320, 320]               0\n",
      "             ReLU-79           [1, 8, 320, 320]               0\n",
      "        Dropout2d-80           [1, 8, 320, 320]               0\n",
      "        ConvBlock-81           [1, 8, 320, 320]               0\n",
      "           Conv2d-82           [1, 4, 320, 320]              36\n",
      "           Conv2d-83           [1, 1, 320, 320]               5\n",
      "           Conv2d-84           [1, 1, 320, 320]               2\n",
      "================================================================\n",
      "Total params: 23,779\n",
      "Trainable params: 23,779\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 192.77\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 193.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_size=(channels, H, W)\n",
    "summary(model, input_size=(1, 320, 320), batch_size=1, device=str(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1714\n",
      "Validating on 420\n",
      "Epoch: 1/20\n",
      " Train Loss: 0.08017883475261241 | Validation Loss: 0.07722513851377855\n",
      "Train Time: 43.912557292031124 | Validation Time: 11.322149015963078\n",
      "Epoch: 2/20\n",
      " Train Loss: 0.07496281865262786 | Validation Loss: 0.07336352825101866\n",
      "Train Time: 44.13256138609722 | Validation Time: 11.37965878401883\n",
      "Epoch: 3/20\n",
      " Train Loss: 0.0735501636566242 | Validation Loss: 0.07268417314344036\n",
      "Train Time: 44.24089343100786 | Validation Time: 11.332404565997422\n",
      "Epoch: 4/20\n",
      " Train Loss: 0.07232924327899953 | Validation Loss: 0.07339268747264399\n",
      "Train Time: 44.77421961806249 | Validation Time: 11.443333951057866\n",
      "Epoch: 5/20\n",
      " Train Loss: 0.07300845130510654 | Validation Loss: 0.07171442144068674\n",
      "Train Time: 44.64151468104683 | Validation Time: 11.472441095975228\n",
      "Epoch: 6/20\n",
      " Train Loss: 0.06942566106052642 | Validation Loss: 0.06938785230150703\n",
      "Train Time: 44.60896102001425 | Validation Time: 11.41010370303411\n",
      "Epoch: 7/20\n",
      " Train Loss: 0.07253728429997751 | Validation Loss: 0.07088930053127465\n",
      "Train Time: 44.54580701596569 | Validation Time: 11.412113614031114\n",
      "Epoch: 8/20\n",
      " Train Loss: 0.06997925315361472 | Validation Loss: 0.06902235104684544\n",
      "Train Time: 44.55725042405538 | Validation Time: 11.41346935403999\n",
      "Epoch: 9/20\n",
      " Train Loss: 0.06989672325961047 | Validation Loss: 0.06822181662519783\n",
      "Train Time: 44.650077097001486 | Validation Time: 11.561940059065819\n",
      "Epoch: 10/20\n",
      " Train Loss: 0.07179642548846614 | Validation Loss: 0.06849540197036806\n",
      "Train Time: 44.5976893909974 | Validation Time: 11.471908556995913\n",
      "Epoch: 11/20\n",
      " Train Loss: 0.06766755668717254 | Validation Loss: 0.07018544668784953\n",
      "Train Time: 44.51809853990562 | Validation Time: 11.45539603906218\n",
      "Epoch: 12/20\n",
      " Train Loss: 0.07174454237526243 | Validation Loss: 0.06897009052683396\n",
      "Train Time: 44.6560026780935 | Validation Time: 11.46185774402693\n",
      "Epoch: 13/20\n",
      " Train Loss: 0.06954287761767554 | Validation Loss: 0.06796926229521996\n",
      "Train Time: 44.6067786939675 | Validation Time: 11.421836794936098\n",
      "Epoch: 14/20\n",
      " Train Loss: 0.07106958427402528 | Validation Loss: 0.0695387542330005\n",
      "Train Time: 44.63530016003642 | Validation Time: 11.439919973956421\n",
      "Epoch: 15/20\n",
      " Train Loss: 0.06736987869416904 | Validation Loss: 0.06748960570809609\n",
      "Train Time: 44.67405280703679 | Validation Time: 11.463432513992302\n",
      "Epoch: 16/20\n",
      " Train Loss: 0.06768307251813846 | Validation Loss: 0.06480798968332989\n",
      "Train Time: 44.77893359796144 | Validation Time: 11.45581488602329\n",
      "Epoch: 17/20\n",
      " Train Loss: 0.06819469164382937 | Validation Loss: 0.06642744521914053\n",
      "Train Time: 44.65809362393338 | Validation Time: 11.424661883036606\n",
      "Epoch: 18/20\n",
      " Train Loss: 0.06791099700439741 | Validation Loss: 0.06573333197771168\n",
      "Train Time: 44.91322911600582 | Validation Time: 11.490021866047755\n",
      "Epoch: 19/20\n",
      " Train Loss: 0.06615059593347442 | Validation Loss: 0.0671437263910664\n",
      "Train Time: 44.97644158790354 | Validation Time: 11.607724888017401\n",
      "Epoch: 20/20\n",
      " Train Loss: 0.06793710047625423 | Validation Loss: 0.06656036691604199\n",
      "Train Time: 44.7989492029883 | Validation Time: 11.49949973996263\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "scheduler = StepLR(optimizer, step_size, lr_gamma)\n",
    "current_epoch = 0\n",
    "# record loss overtime for plotting\n",
    "train_loss_ot = []\n",
    "val_loss_ot = []\n",
    "\n",
    "print(\"Training on \" + str(len(train_data)))\n",
    "print(\"Validating on \" + str(len(val_data)))\n",
    "    \n",
    "# run model epochs\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    scheduler.step(epoch)\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(epochs))\n",
    "    train_loss, train_time = training_epoch(epoch, model, train_loader, optimizer)\n",
    "    val_loss, val_time = validation_epoch(epoch, model, val_loader, optimizer)\n",
    "    train_loss_ot.append(train_loss)\n",
    "    val_loss_ot.append(val_loss)\n",
    "    print(\" Train Loss: \" + str(train_loss) + \" | Validation Loss: \" + str(val_loss))\n",
    "    print(\"Train Time: \" + str(train_time) + \" | Validation Time: \" + str(val_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_ot, label='train')\n",
    "plt.plot(val_loss_ot, label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss' + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-878abefd8181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# img ground truth, img undersampled, raw data understampled, masks, norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawdata_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_und\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_und_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "data_sample = next(iter(train_loader))\n",
    "# img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "pred = model(img_und_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-c6c1d13c8a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_gt_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_gt_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_gt_cropped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# img_und = T.complex_abs(img_und)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/neural_comp_report/functions/transforms.py\u001b[0m in \u001b[0;36mcomplex_abs\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAbsolute\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_gt = T.complex_abs(img_gt)\n",
    "img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "img_gt_2d = img_gt_cropped[-1,:,:]\n",
    "\n",
    "# img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320,320])\n",
    "# to 2s\n",
    "img_und_2d = img_und_cropped[-1,:,:]\n",
    "\n",
    "# bring to cpu\n",
    "predc = pred.cpu().detach()\n",
    "# prediction 4d -> 3d\n",
    "pred3d = predc[-1,:,:,:]\n",
    "# prediction 3d -> 2d\n",
    "pred2d = predc[-1,-1,:,:]\n",
    "\n",
    "print(img_gt_2d.shape)\n",
    "print(img_und_2d.shape)\n",
    "print(pred2d.shape)\n",
    "all_imgs = torch.stack([img_und_2d,img_gt_2d, pred2d], dim=0)\n",
    "show_slices(all_imgs, [0,1,2], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results from training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "preds = []\n",
    "\n",
    "for iter, data_sample in enumerate(train_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "    pred = model(img_und_padded)\n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())\n",
    "\n",
    "for iter, data_sample in enumerate(val_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "    pred = model(img_und_padded)\n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on training data\n",
    "We can evaluate SSIM on the whole volume in the region of interset (320x320 central region) with respect to ground truth. As can be seen, the more aggressive sampling we have, the lower SSIM value we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). Required 3D input np arrays\"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1,2,0), pred.transpose(1,2,0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.4686312791624842\n"
     ]
    }
   ],
   "source": [
    "length = len(gts)\n",
    "i = 0\n",
    "ssim_comb = 0\n",
    "for i in range(0,length):\n",
    "    ssim_comb += ssim(gts[i], preds[i])\n",
    "\n",
    "ssim = ssim_comb / length\n",
    "\n",
    "print(\"Average SSIM: \" + str(ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
