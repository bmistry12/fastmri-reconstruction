{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment Variables and Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from functions import transforms as T\n",
    "from functions.subsample import MaskFunc\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, Sequential, InstanceNorm2d, ReLU, Dropout2d, Module, ModuleList, functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adagrad, Adam, RMSprop, ASGD, SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from scipy.io import loadmat\n",
    "from skimage.measure import compare_ssim \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labs\n",
    "data_path_train = '/data/local/NC2019MRI/train'\n",
    "data_path_test = '/data/local/NC2019MRI/test'\n",
    "\n",
    "# CHANGE OUTPUT DIRECTORY - output directory for test images\n",
    "out_dir = \"/tmp/bhm699/8f/\"\n",
    "\n",
    "# 0.2 = split training dataset into 20% validation data, 80% training data\n",
    "train_val_split = 0.2\n",
    "\n",
    "# for mask 8AF - acc = 8, cen = 0.04\n",
    "acc = 8\n",
    "cen_fract = 0.04\n",
    "\n",
    "seed = True # random masks for each slice \n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "# Model parameters\n",
    "in_chans = 1\n",
    "out_chans = 1\n",
    "chans = 8\n",
    "# This needs to be (1,1) for the model to run...why...\n",
    "kernel_size=(1, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 30\n",
    "dropout_prob = 0.01\n",
    "learning_rate = 0.1\n",
    "weight_decay = 0.0\n",
    "step_size = 15\n",
    "lr_gamma = 0.1 # change in learning rate\n",
    "num_pool_layers = 4\n",
    "\n",
    "# Check if gpu is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path):\n",
    "#     eventually make this random subsets by shuffling data for\n",
    "    \"\"\" Go through training data path, list all file names, the file paths and the slices of subjects. \n",
    "    Split into training and validation set depending on value of train_val_split\n",
    "    \"\"\"\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    \n",
    "    files = len(os.listdir(train_data_path))\n",
    "    train_files_num = (1 - train_val_split) * files\n",
    "\n",
    "    i = 0    \n",
    "    for fname in sorted(os.listdir(train_data_path)):\n",
    "        subject_data_path = os.path.join(train_data_path, fname)\n",
    "        if not os.path.isfile(subject_data_path): continue \n",
    "        \n",
    "        if i <= train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                train_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        elif i > train_files_num:\n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]        \n",
    "                # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                val_files += [(fname, subject_data_path, slice) for slice in range(5, num_slice)]\n",
    "        i += 1\n",
    "        \n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]             \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: \n",
    "        norm = 1e-6\n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Unet: Neural networks with downsampling and upsampling. ref: https://github.com/facebookresearch/fastMRI/blob/master/models/unet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel(Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of a U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.down_sample_layers = ModuleList([ConvBlock(in_chans, chans, drop_prob, kernel_size)])\n",
    "        ch = chans\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.down_sample_layers += [ConvBlock(ch, ch * 2, drop_prob, kernel_size)]\n",
    "            ch *= 2\n",
    "        self.conv = ConvBlock(ch, ch, drop_prob, kernel_size)\n",
    "\n",
    "        self.up_sample_layers = ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            self.up_sample_layers += [ConvBlock(ch * 2, ch // 2, drop_prob, kernel_size)]\n",
    "            ch //= 2\n",
    "        self.up_sample_layers += [ConvBlock(ch * 2, ch, drop_prob, kernel_size)]\n",
    "        self.conv2 = Sequential(\n",
    "            Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.down_sample_layers:\n",
    "            output = layer(output)\n",
    "            stack.append(output)\n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.up_sample_layers:\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Block that consists of two convolution layers each followed by\n",
    "    instance normalization, relu activation and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_chans, out_chans, drop_prob, kernel_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input.\n",
    "            out_chans (int): Number of channels in the output.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(in_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob),\n",
    "            Conv2d(out_chans, out_chans, kernel_size=self.kernel_size),\n",
    "            InstanceNorm2d(out_chans),\n",
    "            ReLU(),\n",
    "            Dropout2d(drop_prob)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args: input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns: (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        return self.layers(input)\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return ConvBlock(in_chans={self.in_chans}, out_chans={self.out_chans}, drop_prob={self.drop_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und)\n",
    "        input = T.center_crop(input, [320, 320])\n",
    "        input = input[None, ...].to(device, dtype=torch.float)\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_epoch(epoch, model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    start_epoch = start_iter = time.perf_counter()\n",
    "    global_step = epoch * len(data_loader)\n",
    "    \n",
    "    for iter, data_sample in enumerate(data_loader):\n",
    "        # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "        img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "        input = T.complex_abs(img_und) # absolute values\n",
    "        input = T.center_crop(input, [320, 320]) # crop to 320  x 320\n",
    "        input = input[None, ...].to(device, dtype=torch.float) # 3d to 4d tensor\n",
    "        \n",
    "        target = T.complex_abs(img_gt)\n",
    "        target = T.center_crop(target, [320, 320])\n",
    "        target = target[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(input)\n",
    "        loss = F.l1_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = 0.99 * avg_loss + 0.01 * loss.item() if iter > 0 else loss.item()\n",
    "        start_iter = time.perf_counter()\n",
    "        \n",
    "    return avg_loss, time.perf_counter() - start_epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':       \n",
    "    train_data, val_data  = load_data_path(data_path_train) # first load all file names, paths and slices.\n",
    "\n",
    "    # create data loader for training and validation sets\n",
    "    train_dataset = MRIDataset(train_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "    val_dataset = MRIDataset(val_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    # create model object\n",
    "    model = UnetModel(in_chans=in_chans, out_chans=out_chans, chans=chans, num_pool_layers=4, drop_prob=dropout_prob, kernel_size=kernel_size).to(device)\n",
    "    # use RMSprop as optimizer\n",
    "    optimizer = SGD(model.parameters(), learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 8, 320, 320]              16\n",
      "    InstanceNorm2d-2           [1, 8, 320, 320]               0\n",
      "              ReLU-3           [1, 8, 320, 320]               0\n",
      "         Dropout2d-4           [1, 8, 320, 320]               0\n",
      "            Conv2d-5           [1, 8, 320, 320]              72\n",
      "    InstanceNorm2d-6           [1, 8, 320, 320]               0\n",
      "              ReLU-7           [1, 8, 320, 320]               0\n",
      "         Dropout2d-8           [1, 8, 320, 320]               0\n",
      "         ConvBlock-9           [1, 8, 320, 320]               0\n",
      "           Conv2d-10          [1, 16, 160, 160]             144\n",
      "   InstanceNorm2d-11          [1, 16, 160, 160]               0\n",
      "             ReLU-12          [1, 16, 160, 160]               0\n",
      "        Dropout2d-13          [1, 16, 160, 160]               0\n",
      "           Conv2d-14          [1, 16, 160, 160]             272\n",
      "   InstanceNorm2d-15          [1, 16, 160, 160]               0\n",
      "             ReLU-16          [1, 16, 160, 160]               0\n",
      "        Dropout2d-17          [1, 16, 160, 160]               0\n",
      "        ConvBlock-18          [1, 16, 160, 160]               0\n",
      "           Conv2d-19            [1, 32, 80, 80]             544\n",
      "   InstanceNorm2d-20            [1, 32, 80, 80]               0\n",
      "             ReLU-21            [1, 32, 80, 80]               0\n",
      "        Dropout2d-22            [1, 32, 80, 80]               0\n",
      "           Conv2d-23            [1, 32, 80, 80]           1,056\n",
      "   InstanceNorm2d-24            [1, 32, 80, 80]               0\n",
      "             ReLU-25            [1, 32, 80, 80]               0\n",
      "        Dropout2d-26            [1, 32, 80, 80]               0\n",
      "        ConvBlock-27            [1, 32, 80, 80]               0\n",
      "           Conv2d-28            [1, 64, 40, 40]           2,112\n",
      "   InstanceNorm2d-29            [1, 64, 40, 40]               0\n",
      "             ReLU-30            [1, 64, 40, 40]               0\n",
      "        Dropout2d-31            [1, 64, 40, 40]               0\n",
      "           Conv2d-32            [1, 64, 40, 40]           4,160\n",
      "   InstanceNorm2d-33            [1, 64, 40, 40]               0\n",
      "             ReLU-34            [1, 64, 40, 40]               0\n",
      "        Dropout2d-35            [1, 64, 40, 40]               0\n",
      "        ConvBlock-36            [1, 64, 40, 40]               0\n",
      "           Conv2d-37            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-38            [1, 64, 20, 20]               0\n",
      "             ReLU-39            [1, 64, 20, 20]               0\n",
      "        Dropout2d-40            [1, 64, 20, 20]               0\n",
      "           Conv2d-41            [1, 64, 20, 20]           4,160\n",
      "   InstanceNorm2d-42            [1, 64, 20, 20]               0\n",
      "             ReLU-43            [1, 64, 20, 20]               0\n",
      "        Dropout2d-44            [1, 64, 20, 20]               0\n",
      "        ConvBlock-45            [1, 64, 20, 20]               0\n",
      "           Conv2d-46            [1, 32, 40, 40]           4,128\n",
      "   InstanceNorm2d-47            [1, 32, 40, 40]               0\n",
      "             ReLU-48            [1, 32, 40, 40]               0\n",
      "        Dropout2d-49            [1, 32, 40, 40]               0\n",
      "           Conv2d-50            [1, 32, 40, 40]           1,056\n",
      "   InstanceNorm2d-51            [1, 32, 40, 40]               0\n",
      "             ReLU-52            [1, 32, 40, 40]               0\n",
      "        Dropout2d-53            [1, 32, 40, 40]               0\n",
      "        ConvBlock-54            [1, 32, 40, 40]               0\n",
      "           Conv2d-55            [1, 16, 80, 80]           1,040\n",
      "   InstanceNorm2d-56            [1, 16, 80, 80]               0\n",
      "             ReLU-57            [1, 16, 80, 80]               0\n",
      "        Dropout2d-58            [1, 16, 80, 80]               0\n",
      "           Conv2d-59            [1, 16, 80, 80]             272\n",
      "   InstanceNorm2d-60            [1, 16, 80, 80]               0\n",
      "             ReLU-61            [1, 16, 80, 80]               0\n",
      "        Dropout2d-62            [1, 16, 80, 80]               0\n",
      "        ConvBlock-63            [1, 16, 80, 80]               0\n",
      "           Conv2d-64           [1, 8, 160, 160]             264\n",
      "   InstanceNorm2d-65           [1, 8, 160, 160]               0\n",
      "             ReLU-66           [1, 8, 160, 160]               0\n",
      "        Dropout2d-67           [1, 8, 160, 160]               0\n",
      "           Conv2d-68           [1, 8, 160, 160]              72\n",
      "   InstanceNorm2d-69           [1, 8, 160, 160]               0\n",
      "             ReLU-70           [1, 8, 160, 160]               0\n",
      "        Dropout2d-71           [1, 8, 160, 160]               0\n",
      "        ConvBlock-72           [1, 8, 160, 160]               0\n",
      "           Conv2d-73           [1, 8, 320, 320]             136\n",
      "   InstanceNorm2d-74           [1, 8, 320, 320]               0\n",
      "             ReLU-75           [1, 8, 320, 320]               0\n",
      "        Dropout2d-76           [1, 8, 320, 320]               0\n",
      "           Conv2d-77           [1, 8, 320, 320]              72\n",
      "   InstanceNorm2d-78           [1, 8, 320, 320]               0\n",
      "             ReLU-79           [1, 8, 320, 320]               0\n",
      "        Dropout2d-80           [1, 8, 320, 320]               0\n",
      "        ConvBlock-81           [1, 8, 320, 320]               0\n",
      "           Conv2d-82           [1, 4, 320, 320]              36\n",
      "           Conv2d-83           [1, 1, 320, 320]               5\n",
      "           Conv2d-84           [1, 1, 320, 320]               2\n",
      "================================================================\n",
      "Total params: 23,779\n",
      "Trainable params: 23,779\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.39\n",
      "Forward/backward pass size (MB): 192.77\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 193.25\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# input_size=(channels, H, W)\n",
    "summary(model, input_size=(1, 320, 320), batch_size=1, device=str(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1714\n",
      "Validating on 420\n",
      "Epoch: 1/30\n",
      " Train Loss: 0.09863579547715877 | Validation Loss: 0.10007570845954478\n",
      "Train Time: 43.44957096292637 | Validation Time: 11.348926160950214\n",
      "Epoch: 2/30\n",
      " Train Loss: 0.09672567011097305 | Validation Loss: 0.09033523829876901\n",
      "Train Time: 43.5167067409493 | Validation Time: 11.328038282925263\n",
      "Epoch: 3/30\n",
      " Train Loss: 0.09108765690060813 | Validation Loss: 0.09076207527864964\n",
      "Train Time: 43.71833837102167 | Validation Time: 11.386125542921945\n",
      "Epoch: 4/30\n",
      " Train Loss: 0.08847831654619413 | Validation Loss: 0.08792777257516542\n",
      "Train Time: 43.76057583093643 | Validation Time: 11.357248899061233\n",
      "Epoch: 5/30\n",
      " Train Loss: 0.08786087947584517 | Validation Loss: 0.08720585183258951\n",
      "Train Time: 43.76574688311666 | Validation Time: 11.319511200999841\n",
      "Epoch: 6/30\n",
      " Train Loss: 0.08597917747172767 | Validation Loss: 0.09124964123221242\n",
      "Train Time: 43.80278263706714 | Validation Time: 11.357174954144284\n",
      "Epoch: 7/30\n",
      " Train Loss: 0.08320106667846337 | Validation Loss: 0.08652725579914294\n",
      "Train Time: 43.76774765201844 | Validation Time: 11.419800205854699\n",
      "Epoch: 8/30\n",
      " Train Loss: 0.08536424579522751 | Validation Loss: 0.08086630729342867\n",
      "Train Time: 43.89053312083706 | Validation Time: 11.353959107073024\n",
      "Epoch: 9/30\n",
      " Train Loss: 0.083578757066764 | Validation Loss: 0.08519984047345315\n",
      "Train Time: 43.95424143993296 | Validation Time: 11.395926949102432\n",
      "Epoch: 10/30\n",
      " Train Loss: 0.0818261722040348 | Validation Loss: 0.08121346974029117\n",
      "Train Time: 43.857165611116216 | Validation Time: 11.446766007924452\n",
      "Epoch: 11/30\n",
      " Train Loss: 0.07957777569905654 | Validation Loss: 0.08144920330201263\n",
      "Train Time: 44.08535317890346 | Validation Time: 11.408390467986465\n",
      "Epoch: 12/30\n",
      " Train Loss: 0.08316979288663134 | Validation Loss: 0.07921547920734695\n",
      "Train Time: 43.896275012986735 | Validation Time: 11.510583936003968\n",
      "Epoch: 13/30\n",
      " Train Loss: 0.08119510938732499 | Validation Loss: 0.08021267074300092\n",
      "Train Time: 44.013639150885865 | Validation Time: 11.40269172587432\n",
      "Epoch: 14/30\n",
      " Train Loss: 0.08167577598570981 | Validation Loss: 0.08052549645671443\n",
      "Train Time: 43.927252450957894 | Validation Time: 11.443051551934332\n",
      "Epoch: 15/30\n",
      " Train Loss: 0.07920915222135835 | Validation Loss: 0.08169597580789667\n",
      "Train Time: 43.92755269398913 | Validation Time: 11.404217194998637\n",
      "Epoch: 16/30\n",
      " Train Loss: 0.07493837534062851 | Validation Loss: 0.07385314950680902\n",
      "Train Time: 43.99105927511118 | Validation Time: 11.400513030122966\n",
      "Epoch: 17/30\n",
      " Train Loss: 0.0769201103188181 | Validation Loss: 0.07466882137790026\n",
      "Train Time: 44.04912007204257 | Validation Time: 11.417842254042625\n",
      "Epoch: 18/30\n",
      " Train Loss: 0.07559785061897882 | Validation Loss: 0.07246870491514121\n",
      "Train Time: 43.97600715002045 | Validation Time: 11.416813825024292\n",
      "Epoch: 19/30\n",
      " Train Loss: 0.07444506890833444 | Validation Loss: 0.07302717321415722\n",
      "Train Time: 43.93058684002608 | Validation Time: 11.437915317947045\n",
      "Epoch: 20/30\n",
      " Train Loss: 0.07437555408153688 | Validation Loss: 0.07462978476720727\n",
      "Train Time: 44.026634274981916 | Validation Time: 11.415013157995418\n",
      "Epoch: 21/30\n",
      " Train Loss: 0.0746816361109549 | Validation Loss: 0.07300227306699006\n",
      "Train Time: 43.912746499991044 | Validation Time: 11.436842242954299\n",
      "Epoch: 22/30\n",
      " Train Loss: 0.07272011711168706 | Validation Loss: 0.07377390606597461\n",
      "Train Time: 43.87861374299973 | Validation Time: 11.438644986832514\n",
      "Epoch: 23/30\n",
      " Train Loss: 0.07322668660221296 | Validation Loss: 0.0737351811959107\n",
      "Train Time: 43.94163477886468 | Validation Time: 11.3788537948858\n",
      "Epoch: 24/30\n",
      " Train Loss: 0.0717953200104338 | Validation Loss: 0.07154711039142782\n",
      "Train Time: 43.860710143111646 | Validation Time: 11.422848721966147\n",
      "Epoch: 25/30\n",
      " Train Loss: 0.07374700142683512 | Validation Loss: 0.07422760764920812\n",
      "Train Time: 44.1968430520501 | Validation Time: 11.635784745914862\n",
      "Epoch: 26/30\n",
      " Train Loss: 0.07246601052781004 | Validation Loss: 0.0743601876121829\n",
      "Train Time: 44.217455110978335 | Validation Time: 11.412954400060698\n",
      "Epoch: 27/30\n",
      " Train Loss: 0.07246594523226926 | Validation Loss: 0.07261388299524835\n",
      "Train Time: 43.92528959317133 | Validation Time: 11.426159139024094\n",
      "Epoch: 28/30\n",
      " Train Loss: 0.07497549147726429 | Validation Loss: 0.07257190963427759\n",
      "Train Time: 43.9316499279812 | Validation Time: 11.441642748890445\n",
      "Epoch: 29/30\n",
      " Train Loss: 0.07336802330407872 | Validation Loss: 0.07252568050352794\n",
      "Train Time: 43.92484694207087 | Validation Time: 11.43243286316283\n",
      "Epoch: 30/30\n",
      " Train Loss: 0.07630648326060156 | Validation Loss: 0.07177936616278517\n",
      "Train Time: 43.94271681807004 | Validation Time: 11.44872503192164\n"
     ]
    }
   ],
   "source": [
    "# Epochs\n",
    "scheduler = StepLR(optimizer, step_size, lr_gamma)\n",
    "current_epoch = 0\n",
    "# record loss overtime for plotting\n",
    "train_loss_ot = []\n",
    "val_loss_ot = []\n",
    "\n",
    "print(\"Training on \" + str(len(train_data)))\n",
    "print(\"Validating on \" + str(len(val_data)))\n",
    "    \n",
    "# run model epochs\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    scheduler.step(epoch)\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(epochs))\n",
    "    train_loss, train_time = training_epoch(epoch, model, train_loader, optimizer)\n",
    "    val_loss, val_time = validation_epoch(epoch, model, val_loader, optimizer)\n",
    "    train_loss_ot.append(train_loss)\n",
    "    val_loss_ot.append(val_loss)\n",
    "    print(\" Train Loss: \" + str(train_loss) + \" | Validation Loss: \" + str(val_loss))\n",
    "    print(\"Train Time: \" + str(train_time) + \" | Validation Time: \" + str(val_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_ot, label='train')\n",
    "plt.plot(val_loss_ot, label='validation')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig('loss-8f-' + str(epochs) + 'sgd.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-be918f45b990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# img ground truth, img undersampled, raw data understampled, masks, norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawdata_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_und\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_und_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_und\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "data_sample = next(iter(train_loader))\n",
    "# img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "\n",
    "img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "pred = model(img_und_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-f85da3b26baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_gt_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_gt_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_gt_cropped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# img_und = T.complex_abs(img_und)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/neural_comp_report/functions/transforms.py\u001b[0m in \u001b[0;36mcomplex_abs\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAbsolute\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_gt = T.complex_abs(img_gt)\n",
    "img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "img_gt_2d = img_gt_cropped[-1,:,:]\n",
    "\n",
    "# img_und = T.complex_abs(img_und)\n",
    "img_und_cropped = T.center_crop(img_und, [320,320])\n",
    "# to 2d\n",
    "img_und_2d = img_und_cropped[-1,:,:]\n",
    "\n",
    "# bring to cpu\n",
    "predc = pred.cpu().detach()\n",
    "# prediction 4d -> 3d\n",
    "pred3d = predc[-1,:,:,:]\n",
    "# prediction 3d -> 2d\n",
    "pred2d = predc[-1,-1,:,:]\n",
    "\n",
    "print(img_gt_2d.shape)\n",
    "print(img_und_2d.shape)\n",
    "print(pred2d.shape)\n",
    "all_imgs = torch.stack([img_und_2d,img_gt_2d, pred2d], dim=0)\n",
    "show_slices(all_imgs, [0,1,2], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results from training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "preds = []\n",
    "\n",
    "for iter, data_sample in enumerate(train_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "    img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "    \n",
    "    pred = model(img_und_normalised)\n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())\n",
    "\n",
    "for iter, data_sample in enumerate(val_loader):\n",
    "    # img ground truth, img undersampled, raw data understampled, masks, norm\n",
    "    img_gt, img_und, rawdata_und, masks, norm = data_sample\n",
    "    img_und = T.complex_abs(img_und)\n",
    "    img_und_cropped = T.center_crop(img_und, [320, 320])\n",
    "    img_und_padded = img_und_cropped[None, ...].to(device, dtype=torch.float)\n",
    "    \n",
    "    img_und_normalised = img_und_padded * norm.to(device, dtype=torch.float) \n",
    "\n",
    "    img_gt = T.complex_abs(img_gt)\n",
    "    img_gt_cropped = T.center_crop(img_gt, [320,320])\n",
    "\n",
    "    # bring to cpu\n",
    "    predc = pred.cpu().detach()\n",
    "    # prediction 4d -> 3d\n",
    "    pred3d = predc[-1,:,:,:]\n",
    "    # prediction 3d -> 2d\n",
    "    pred2d = predc[-1,-1,:,:]\n",
    "    \n",
    "    gts.append(img_gt_cropped.numpy())\n",
    "    preds.append(pred3d.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on training data\n",
    "We can evaluate SSIM on the whole volume in the region of interset (320x320 central region) with respect to ground truth. As can be seen, the more aggressive sampling we have, the lower SSIM value we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). Required 3D input np arrays\"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1,2,0), pred.transpose(1,2,0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM: 0.38620382100528033\n"
     ]
    }
   ],
   "source": [
    "length = len(gts)\n",
    "i = 0\n",
    "ssim_comb = 0\n",
    "for i in range(0,length):\n",
    "    ssim_comb += ssim(gts[i], preds[i])\n",
    "\n",
    "ssim = ssim_comb / length\n",
    "\n",
    "print(\"Average SSIM: \" + str(ssim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run against Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MRITestDataset(DataLoader):\n",
    "#     def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "#         self.data_list = data_list\n",
    "#         self.acceleration = acceleration\n",
    "#         self.center_fraction = center_fraction\n",
    "#         self.use_seed = use_seed\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_list)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         subject_id = self.data_list[idx]\n",
    "#         return get_test_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_batch(subject_id, acc, center_fract, use_seed):\n",
    "#     ''' random select a few slices (batch_size) from each volume'''\n",
    "#     fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "#     with h5py.File(rawdata_name, 'r') as data:\n",
    "#         rawdata = data['kspace_8af'][slice]             \n",
    "#         slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "#         S, Ny, Nx, ps = slice_kspace.shape\n",
    "    \n",
    "#     # apply random mask\n",
    "#     shape = np.array(slice_kspace.shape)\n",
    "#     mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "#     seed = None if not use_seed else tuple(map(ord, fname))\n",
    "#     mask = mask_func(shape, seed)\n",
    "      \n",
    "#     # undersample\n",
    "#     masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "#     masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "#     img_und = T.ifft2(masked_kspace)\n",
    "#     # perform data normalization which is important for network to learn useful features\n",
    "#     # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "#     norm = T.complex_abs(img_und).max()\n",
    "#     if norm < 1e-6: \n",
    "#         norm = 1e-6\n",
    "#     # normalized data\n",
    "#     img_und, rawdata_und = img_und/norm, masked_kspace/norm\n",
    "        \n",
    "#     return img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_reconstructions(reconstruction, fname):\n",
    "#     \"\"\"\n",
    "#     Saves the reconstructions from a model into h5 files that is appropriate for submission\n",
    "#     to the leaderboard.\n",
    "#     \"\"\"\n",
    "#     if not (os.path.exists(out_dir)): os.makedirs(out_dir)\n",
    "#     subject_path = os.path.join(out_dir, fname)\n",
    "#     print(subject_path)\n",
    "#     with h5py.File(subject_path, 'w') as f:\n",
    "#         f.create_dataset('reconstruction', data=reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load test data\n",
    "# test_data = []\n",
    "\n",
    "# for fname in sorted(os.listdir(data_path_test)):\n",
    "#     subject_path = os.path.join(data_path_test, fname)\n",
    "#     if not os.path.isfile(subject_path): continue \n",
    "        \n",
    "#     with h5py.File(subject_path,  \"r\") as hf:\n",
    "#         num_slice_4f = hf['kspace_8af'].shape[0]\n",
    "#         mask_4f = hf['mask_8af']\n",
    "\n",
    "#         test_data += [(fname, subject_path, slice) for slice in range(5, num_slice_4f)]\n",
    "#         # create data loader \n",
    "#         test_dataset = MRITestDataset(test_data, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "\n",
    "#         test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=num_workers) \n",
    "\n",
    "#         test_preds = []\n",
    "\n",
    "#         print(len(test_loader))\n",
    "#         for iter, data_sample in enumerate(test_loader):\n",
    "#             # img_und , raw data undersampled, mask, norm\n",
    "#             volume_kspace_4af, rawdata_und, mask_4af, norm = data_sample\n",
    "#             volume_image_abs = T.complex_abs(volume_kspace_4af)   # Compute absolute value to get a real image\n",
    "\n",
    "#             volume_image_cropped = T.center_crop(volume_image_abs, [320, 320])    \n",
    "#             volume_image_padded = volume_image_cropped[None, ...].to(device, dtype=torch.float)\n",
    "#             volume_image_normalised = volume_image_padded * norm.to(device, dtype=torch.float) \n",
    "\n",
    "#             pred = model(volume_image_normalised)\n",
    "\n",
    "#             # bring to cpu\n",
    "#             predc = pred.cpu().detach()\n",
    "#             # prediction 4d -> 3d\n",
    "#             pred3d = predc[-1,:,:,:]\n",
    "#             # prediction 3d -> 2d\n",
    "#             pred2d = predc[-1,-1,:,:]\n",
    "#             test_preds.append(pred2d)\n",
    "\n",
    "#         predvol = torch.stack(test_preds, dim=0)\n",
    "#         show_slices(predvol, [0,1], cmap='gray')\n",
    "#         plt.pause(1)\n",
    "#         save_reconstructions(predvol, fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
